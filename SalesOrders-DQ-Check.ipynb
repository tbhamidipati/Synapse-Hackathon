{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import sys\r\n",
        "import datetime, time\r\n",
        "from pyspark.sql import SparkSession\r\n",
        "from pyspark import SparkContext, SparkConf\r\n",
        "from pyspark.sql import functions as F\r\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\r\n",
        "\r\n",
        "\r\n",
        "def get_null_perc(spark, df, null_cols):\r\n",
        "    \"\"\" Get null/empty percentage for columns\r\n",
        "\r\n",
        "    Args:\r\n",
        "        spark (Spark): SparkSession object\r\n",
        "        df (DataFrame): dataframe to perform null/empty analysis on\r\n",
        "        null_cols (List): list of columns that need to be considered for analysis\r\n",
        "\r\n",
        "    Returns:\r\n",
        "        DataFrame: dataframe with null check analysis\r\n",
        "    \"\"\"\r\n",
        "    schema = StructType([ \\\r\n",
        "        StructField(\"Column\",StringType(),True), \\\r\n",
        "        StructField(\"NullPercentage\",StringType(),True)\r\n",
        "      ])\r\n",
        "    emptyRDD = spark.sparkContext.emptyRDD()\r\n",
        "    resultdf = spark.createDataFrame(emptyRDD, schema=schema)\r\n",
        "    \r\n",
        "    for x in null_cols:\r\n",
        "    \tdf_null_count = df.select(F.col(x)).filter(F.col(x).isNull() | (F.col(x) == '')).count()\r\n",
        "    \tdf_null = spark.createDataFrame([[x, str(df_null_count*100.0/df.count()) + '%' ]],schema=schema)\r\n",
        "    \tresultdf = resultdf.union(df_null)\r\n",
        "\r\n",
        "    return resultdf\r\n",
        "\r\n",
        "def get_summary_numeric(df, numeric_cols):\r\n",
        "    \"\"\" Get Summary for numeric columns\r\n",
        "\r\n",
        "    Args:\r\n",
        "        df (DataFrame): dataframe to perform analysis on\r\n",
        "        numeric_cols (List): list of columns that need to be considered for analysis\r\n",
        "\r\n",
        "    Returns:\r\n",
        "        DataFrame: dataframe with summary analysis\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    return df.select(numeric_cols).summary()\r\n",
        "\r\n",
        "def get_distinct_counts(spark, df, aggregate_cols):\r\n",
        "    \"\"\" Get distinct count for columns\r\n",
        "\r\n",
        "    Args:\r\n",
        "        spark (Spark): SparkSession object\r\n",
        "        df (DataFrame): dataframe to perform distinct count analysis on\r\n",
        "        aggregate_cols (List): list of columns that need to be considered for analysis\r\n",
        "\r\n",
        "    Returns:\r\n",
        "        DataFrame: dataframe with distinct count analysis\r\n",
        "    \"\"\"\r\n",
        "    schema = StructType([ \\\r\n",
        "        StructField(\"Column\",StringType(),True), \\\r\n",
        "        StructField(\"DistinctCount\",StringType(),True)\r\n",
        "      ])\r\n",
        "    \r\n",
        "    emptyRDD = spark.sparkContext.emptyRDD()\r\n",
        "    resultdf = spark.createDataFrame(emptyRDD, schema=schema)\r\n",
        "    \r\n",
        "    for x in aggregate_cols:\r\n",
        "    \tdf_distinct_count = df.select(F.col(x)).distinct().count()\r\n",
        "    \tdf_distinct = spark.createDataFrame([[x, str(df_distinct_count)]],schema=schema)\r\n",
        "    \tresultdf = resultdf.union(df_distinct)\r\n",
        "\r\n",
        "    return resultdf\r\n",
        "\r\n",
        "def get_distribution_counts(spark, df, aggregate_cols):\r\n",
        "    \"\"\" Get Distribution Counts for columns\r\n",
        "\r\n",
        "    Args:\r\n",
        "        spark (Spark): SparkSession object\r\n",
        "        df (DataFrame): dataframe to perform null/empty analysis on\r\n",
        "        aggregate_cols (List): list of columns that need to be considered for analysis\r\n",
        "\r\n",
        "    Returns:\r\n",
        "        Array: Array of objects with dataframes\r\n",
        "    \"\"\"\r\n",
        "    result = []\r\n",
        "    for i in aggregate_cols:\r\n",
        "    \tresult.append(df.groupby(F.col(i)).count().sort(F.col(\"count\").desc()))\r\n",
        "    ###\r\n",
        "    \r\n",
        "    return result\r\n",
        "\r\n",
        "def get_mismatch_perc(spark, df, data_quality_cols_regex):\r\n",
        "    \"\"\" Get Mismatch Percentage for columns\r\n",
        "\r\n",
        "    Args:\r\n",
        "        spark (Spark): SparkSession object\r\n",
        "        df (DataFrame): dataframe to perform null/empty analysis on\r\n",
        "        data_quality_cols_regex (Dictionary): Dictionary of columns/regex-expression for data quality analysis\r\n",
        "\r\n",
        "    Returns:\r\n",
        "        DataFrame: DataFrame with data quality analysis\r\n",
        "    \"\"\"\r\n",
        "    schema = StructType([ \\\r\n",
        "        StructField(\"Column\",StringType(),True), \\\r\n",
        "        StructField(\"MismatchPercentage\",StringType(),True)\r\n",
        "      ])\r\n",
        "    \r\n",
        "    emptyRDD = spark.sparkContext.emptyRDD()\r\n",
        "    resultdf = spark.createDataFrame(emptyRDD, schema=schema)\r\n",
        "    \r\n",
        "    \r\n",
        "    for key, value in data_quality_cols_regex.items():\r\n",
        "    \tdf_regex_not_like_count = df.select(F.col(key)).filter(~F.col(key).rlike(value)).count()\r\n",
        "    \tdf_regex_not_like = spark.createDataFrame([[key, str(df_regex_not_like_count*100.0/df.count()) + '%']],schema=schema)\r\n",
        "    \tresultdf = resultdf.union(df_regex_not_like)\r\n",
        "    \r\n",
        "    return resultdf"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "SparkPool01",
              "session_id": 12,
              "statement_id": 1,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2022-06-01T16:31:58.0471038Z",
              "session_start_time": "2022-06-01T16:31:58.1014862Z",
              "execution_start_time": "2022-06-01T16:32:51.5008677Z",
              "execution_finish_time": "2022-06-01T16:32:51.9560072Z"
            },
            "text/plain": "StatementMeta(SparkPool01, 12, 1, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 1,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load tables for data quality checks \r\n",
        "\r\n",
        "df = spark.read.load('<ADLG2>', format='format'\r\n",
        "## If header exists uncomment line below\r\n",
        ", header=True\r\n",
        ")\r\n",
        "display(df.limit(10))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "microsoft": {},
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This is a master schema of all columns name and type. below is an example for Sales.Orders Table\r\n",
        "schema = StructType([\r\n",
        "\t\t \tStructField(name='OrderID', dataType=IntegerType(), nullable=False),\r\n",
        "\t\t \tStructField(name='CustomerID', dataType=IntegerType(), nullable=False),\r\n",
        "\t\t \tStructField(name='SalespersonPersonID', dataType=IntegerType(), nullable=True),\r\n",
        "            StructField(name='PickedByPersonID', dataType=IntegerType(), nullable=True),\r\n",
        "            StructField(name='ContactPersonID', dataType=IntegerType(), nullable=True),\r\n",
        "            StructField(name='BackorderOrderID', dataType=IntegerType(), nullable=True),\r\n",
        "\t\t \tStructField(name='OrderDate', dataType=DateType(), nullable=False),\r\n",
        "\t\t \tStructField(name='ExpectedDeliveryDate', dataType=DateType(), nullable=True),\r\n",
        "\t\t \tStructField(name='CustomerPurchaseOrderNumber', dataType=StringType(), nullable=True),\r\n",
        "            StructField(name='IsUndersupplyBackordered', dataType=StringType(), nullable=True),\r\n",
        "            StructField(name='Comments', dataType=StringType(), nullable=True),\r\n",
        "\t\t \tStructField(name='DeliveryInstructions', dataType=StringType(), nullable=True),\r\n",
        "            StructField(name='InternalComments', dataType=StringType(), nullable=True),\r\n",
        "\t\t \tStructField(name='PickingCompletedWhen', dataType=TimestampType(), nullable=True),\r\n",
        "\t\t \tStructField(name='LastEditedBy', dataType=IntegerType(), nullable=True),\r\n",
        "\t\t \tStructField(name='LastEditedWhen', dataType=TimestampType(), nullable=True)\r\n",
        "\t\t \t])"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "null_cols = ['Email']\r\n",
        "numeric_cols = ['Phone']\r\n",
        "aggregate_cols = ['Email']\r\n",
        "data_quality_cols_regex = {'age': '^[0-99]{1,2}$', 'first_name': '^[a-zA-Z]*$', 'gender': '^M(ale)?$|^F(emale)?$', 'Email':'/^([a-zA-Z0-9._%-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,6})*$/'}\r\n",
        "result_limit = 10"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "SparkPool01",
              "session_id": 12,
              "statement_id": 11,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2022-06-01T16:39:05.7532549Z",
              "session_start_time": null,
              "execution_start_time": "2022-06-01T16:39:06.1475187Z",
              "execution_finish_time": "2022-06-01T16:39:06.5738363Z"
            },
            "text/plain": "StatementMeta(SparkPool01, 12, 11, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 11,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "null_cols = ['OrderID','CustomerID','OrderDate']\r\n",
        "numeric_cols = ['OrderID', 'CustomerID','SalespersonPersonID','PickedByPersonID','ContactPersonID','BackorderOrderID']\r\n",
        "aggregate_cols = ['OrderID',]\r\n",
        "#data_quality_cols_regex = {'age': '^[0-99]{1,2}$', 'first_name': '^[a-zA-Z]*$', 'gender': '^M(ale)?$|^F(emale)?$', 'Email':'/^([a-zA-Z0-9._%-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,6})*$/'}\r\n",
        "data_quality_cols_regex = {'Gender': '^M(ale)?$|^F(emale)?$', 'Email':'/^([a-zA-Z0-9._%-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,6})*$/'}\r\n",
        "result_limit = 10"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "SparkPool01",
              "session_id": 12,
              "statement_id": 15,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2022-06-01T16:40:35.0828718Z",
              "session_start_time": null,
              "execution_start_time": "2022-06-01T16:40:35.5081869Z",
              "execution_finish_time": "2022-06-01T16:40:35.9546729Z"
            },
            "text/plain": "StatementMeta(SparkPool01, 12, 15, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 15,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### 1. NULL Checks\r\n",
        "resultdf = get_null_perc(spark, df, null_cols)\r\n",
        "print(\"NULL/Empty Percentage for Columns\")\r\n",
        "resultdf.show(result_limit, False)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###2. Summary, Average, Standard Deviation, Percentiles for Numeric Columns\r\n",
        "resultdf = get_summary_numeric(df, numeric_cols)\r\n",
        "print(\"Summary for Numeric Columns\")\r\n",
        "resultdf.show(result_limit, False)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###3. Distinct Count\r\n",
        "print(\"Distinct Counts for Aggregate Columns\")\r\n",
        "resultdf = get_distinct_counts(spark, df, aggregate_cols)\r\n",
        "resultdf.show(result_limit, False)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\r\n",
        "###4. Distribution Count\r\n",
        "print(\"Distribution Count for Aggregate Columns\")\r\n",
        "result = get_distribution_counts(spark, df, aggregate_cols)\r\n",
        "for i in result:\r\n",
        "\tprint(\"======== Distribution for - \" + i.columns[0] + \" ========\")\r\n",
        "\ti.show(result_limit, False)\r\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###5. Data Quality\r\n",
        "print(\"Data Quality Issue Percentage for Columns\")\r\n",
        "resultdf = get_mismatch_perc(spark, df, data_quality_cols_regex)\r\n",
        "resultdf.show(result_limit, False)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "language": "Python",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}